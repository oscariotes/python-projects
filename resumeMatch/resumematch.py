# -*- coding: utf-8 -*-
"""resumeMatch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1awiqR8fLNSnU4QUp5ODI14DY9PwqC3Yc
"""

import os
import nltk
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
from nltk.stem.wordnet import WordNetLemmatizer
from wordcloud import WordCloud

!pip install wordcloud

resume_path = 'larry.txt'
job_description_path = 'desc.txt'

def read_files_to_list(resume_path, job_description_path):
    content_list = []

    # Read content of resume file
    with open(resume_path, 'r') as resume_file:
        content_list.append(resume_file.read().lower().strip())

    # Read content of job description file
    with open(job_description_path, 'r') as job_description_file:
        content_list.append(job_description_file.read().lower().strip())

    return content_list

# Call the function with the file paths
resume_list = read_files_to_list(resume_path, job_description_path)

resume = resume_list[0]
job_description = resume_list[1]

print(resume)
print(job_description)

# Get the list of English stop words
stop_words = stopwords.words('english')

# Print the list of stop words
#print(stop_words)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# # Initialize CountVectorizer with stop words

# removed_words = CountVectorizer(stop_words=stop_words)

# # Fit and transform the corpus
# X = vectorizer.fit_transform(resume_list)

# # Get the feature names
# feature_names = vectorizer.get_feature_names_out(resume_list)

# # Print the transformed matrix and corresponding features
# print("Transformed Matrix:")
# print(X.toarray())
# print("\nFeature Names:")
# print(feature_names)
# print('-----------------------')



vectorizer = CountVectorizer()
vectorized_text = vectorizer.fit_transform(resume_list)





# Calculate cosine similarity
similarity_score = cosine_similarity(vectorized_text)[0][1]*100
similarity_score = round(similarity_score, 2)


# This the cosine similarity score


print('Similarity score:', str(similarity_score)+ '%')

# For job description only

with open(job_description_path, 'r') as f:
  jobdesc = f.read().lower().strip()

# Tokenize sentence
nltk.download('punkt')

tokenized_words = word_tokenize(jobdesc)

# imported stopwords

nltk.download('stopwords')

# Remove stop words from Job description
stop_words = set (stopwords.words('english'))

stop_words.update([':', ',', '(', ')', '&', '*', '.','-', '*',])



filter_jd = []
for word in tokenized_words:
  if word.lower() not in stop_words:
    filter_jd.append(word)

print('Filtered Job Description:', filter_jd)

filter_jd

nltk.download('averaged_perceptron_tagger')
tagged_words = nltk.pos_tag(filter_jd)
for i in tagged_words:
  print(i)



filtered_tag = []

for one in tagged_words:
  if one[1] == 'NN'or one[1] == 'NNS' or one[1] == 'NNP' or one[1] == 'NNPS' or one[1] == 'JJ' or one[1] == 'JJR' or one[1] == 'JJS' or one[1] == 'VB' or one[1] == 'VBD' or one[1] == 'VBG' or one[1] == 'VBN' or one[1] == 'VBP' or one[1] == 'VBZ':
    filtered_tag.append(one)

filtered_tag

fdist_pos = nltk.FreqDist(filter_jd)
top_50_words = fdist_pos.most_common(10)
fdist_pos.plot(50, cumulative=False)
#plt.figure(figsize= (12, 12 ))
plt.show()

for_wordcloud = ' '
for words in filter_jd:
  for_wordcloud = for_wordcloud + words + ' '

for_wordcloud

wordcloud = WordCloud(width=800, height=800, stopwords = stop_words).generate(for_wordcloud)
plt.figure(figsize= (12, 12 ))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

import nltk
from wordcloud import WordCloud, STOPWORDS
from nltk.util import ngrams
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from nltk import ngrams, sent_tokenize

def find_ngrams(text, n=2):
    bigrams = []
    text = sent_tokenize(text)
    for line in text:
        token = nltk.word_tokenize(line)
        bigram = list(ngrams(token, n))
        bigrams.extend(bigram)
    return bigrams

# Remove stop words from Job description
stop_words = set (stopwords.words('english'))

stop_words.update([':', ',', '(', ')', '&', '*', '.','-', '*',])
filtered_text = [x.lower() for x in jobdesc.split() if x.lower() not in stop_words ]
newText = find_ngrams(' '.join(filtered_text))

ngrams_frequency = {}
for i in newText:
  if i not in ngrams_frequency:
    ngrams_frequency[i]=0
  else:
    ngrams_frequency[i]+=1
sorted_ngrams_frequency = sorted(ngrams_frequency.items(), key= lambda x: x[1], reverse=True)
print(sorted_ngrams_frequency)

ngram_joins = []
for i in range(len(sorted_ngrams_frequency[:50])):
  ngram_join = '_'.join(sorted_ngrams_frequency[i][0])
  print(ngram_join)
  ngram_joins.append(ngram_join)

ngram_joins=' '.join(ngram_joins)

wordcloud = WordCloud(width=800, height=800, stopwords = stop_words).generate(ngram_joins)

plt.figure(figsize= (12, 12 ))

plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

print('Similarity score:', str(similarity_score)+ '%')

# Generate N-grams from job description
job_ngrams = find_ngrams(jobdesc)

full_ngrams = find_ngrams(resume)

# Filter N-grams containing resume keywords
filtered_ngrams = [ngram for ngram in job_ngrams if any(keyword in ngram for keyword in resume)]

# Analyze the filtered N-grams using FreqDist or other methods
filtered_ngram_dist = nltk.FreqDist(filtered_ngrams)

# Print or further process the most frequent filtered N-grams
print("Top 10 filtered N-grams:", filtered_ngram_dist.most_common(15))

# Optionally, visualize or compare these N-grams with the full N-gram analysis

# Count the total number of N-grams in the job description
total_ngrams_count = len(full_ngrams)

# Count the number of filtered N-grams (containing keywords)
filtered_ngrams_count = len(filtered_ngrams)

# Calculate the skill overlap percentage
skill_overlap_pct = (filtered_ngrams_count / total_ngrams_count) * 100

# Print the result
print(f"Skill overlap percentage: {skill_overlap_pct:.2f}%")